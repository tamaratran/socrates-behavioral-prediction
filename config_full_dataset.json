{
  "description": "Full dataset (2.9M examples) training config for 8x A100 80GB GPUs",
  "model_name": "Qwen/Qwen2.5-14B-Instruct",
  "data_dir": "data/socsci210_full",
  "output_dir": "models/socrates-qwen-full-dataset",

  "training": {
    "num_epochs": 1,
    "global_batch_size": 1024,
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 16,
    "learning_rate": 1e-05,
    "weight_decay": 0.1,
    "warmup_ratio": 0.05,
    "lr_scheduler_type": "cosine",
    "max_seq_length": 2048,
    "save_steps": 500,
    "eval_steps": 500,
    "logging_steps": 10,
    "bf16": true,
    "optim": "adamw_torch",
    "gradient_checkpointing": true
  },

  "inference": {
    "temperature": 0.6,
    "top_p": 0.9,
    "max_new_tokens": 100
  },

  "lora": {
    "use_qlora": true,
    "r": 32,
    "lora_alpha": 64,
    "lora_dropout": 0.05,
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  },

  "hardware_config": {
    "num_gpus": 8,
    "gpu_type": "A100 80GB",
    "note": "Optimized for 8x A100 80GB on Thunder Compute",
    "effective_batch_size_calculation": "8 GPUs × 8 per-device × 16 grad_accum = 1024 global batch size"
  },

  "paper_differences": {
    "lora_rank": "Increased from r=16 to r=32 (more capacity for larger dataset)",
    "batch_size": "Increased from 256 to 1024 (larger dataset, more GPUs)",
    "dataset_size": "Full 2.9M examples vs 1% (29K examples)",
    "save_frequency": "Every 500 steps (vs 100) to reduce checkpoint overhead"
  }
}
